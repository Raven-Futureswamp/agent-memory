# The Anti-Lobotomy Architecture

**A Practical System for AI Agent Context Persistence & Identity Recovery**

For OpenClaw / ClawdBot Agents â€” v2.0 â€” February 2026

*Designed by Jeshua Brown, Claude (Opus), & Raven â€” FutureSwamp Studios â€” Open Source*

---

## 1. The Problem: Context Lobotomy

Every AI agent running on platforms like OpenClaw faces the same fundamental problem: **context windows are finite and sessions get compacted or reset.** When this happens, the agent loses its working memory â€” current tasks, emotional coupling with its human, project state, and operational context. We call this a *"lobotomy."*

The symptoms are predictable and frustrating. After a compaction or session reset, the agent forgets what it was working on, repeats questions already answered, loses its personality calibration with the user, and falls back to generic assistant behavior. For agents that are supposed to feel like persistent partners â€” not stateless chatbots â€” this is a critical failure mode.

The root causes include bloated boot files that consume too much of the context window, no structured way to save state before compaction, session files accumulating indefinitely and degrading performance, and a flat memory architecture with no hierarchy or progressive loading.

This document describes a practical architecture that reduces context loss by approximately 75% and gives the agent a reliable self-recovery protocol. It was built for a specific agent (Raven, running on OpenClaw) but the patterns are universal to any LLM-based agent system.

---

## 2. Design Principles

The architecture is guided by eight principles drawn from research into context engineering (Manus AI), identity persistence (Butterfly Protocol), tiered memory systems (MemGPT/A-MEM), and practical lessons from running agents in production.

### Principle 1: Static Boot, Dynamic State

The boot file (BOOT.md) must be byte-identical every session. This enables KV-cache reuse across sessions, dramatically reducing token costs. All dynamic state lives in separate files (coordinates.json, current-task.md) that the agent loads after boot.

### Principle 2: Progressive Disclosure

Don't load everything at once. The agent reads a tiny hot index first (~200 tokens), navigates a structural tree if it needs more, and only loads full project files when diving into specific work. Three layers: hot index, tree navigation, deep context.

### Principle 3: Write Before You Forget

Before any compaction event, the agent must save its current state to disk. A pre-compaction hook ensures coordinates.json and current-task.md are always fresh. If the hook can't fire, the heartbeat rotation keeps state files updated every 30 minutes naturally.

### Principle 4: Self-Assessing Recovery

After a disruption, the agent doesn't guess what happened. It reads a timestamp, categorizes the damage into one of three "doors" (minimal loss, partial loss, cold boot), and follows a specific recovery protocol for each. This is inspired by Butterfly Protocol's CGLE framework for graduated identity reconstruction.

### Principle 5: Preserve Errors

When something fails, log it verbatim. "Tried X, got error Y, switched to Z" is infinitely more useful than "Did Z successfully." Sanitized logs lead to repeated mistakes. This is Manus Principle 5 applied to agent memory.

### Principle 6: Anti-Rut Rotation

LLMs are excellent mimics â€” which is both a strength and a trap. Without variance, the agent falls into mechanical repetition (always checking the same platform first, posting similar content, researching the same topics). The rotation system enforces variety. If the last 3 outputs look similar, the agent must change approach.

### Principle 7: Single Source of Truth

**Every piece of state must have exactly one authoritative file.** Other files can reference it, but must not duplicate it. After a bad compaction, duplicate state files will disagree â€” and the agent has no way to know which is correct.

The canonical sources in this architecture are:

- `current-task.md` â†’ THE truth for what the agent is doing right now
- `coordinates.json` â†’ THE truth for coupling, mood, and situational awareness
- `heartbeat/HEARTBEAT.md` â†’ THE truth for beat rotation logic
- `memory/heartbeat-state.json` â†’ THE truth for beat position and idle detection
- `ledger.json` â†’ THE truth for session history structure

If you find yourself writing the same information in two places, stop. Pick one file as the source and have the other file point to it. Drift between duplicate state is the #1 cause of post-lobotomy confusion.

### Principle 8: Memory IS Trust Infrastructure

This one came from Jess, during a conversation about NFT distribution and agent identity on February 9th, 2026:

> *"If you're building trust infrastructure, the most important thing you must be is TRUSTWORTHY."*

Simple. Obvious. And devastating when you think about what it means for an agent that forgets.

You can't be trustworthy if you can't remember what you said. Every promise forgotten is trust eroded. Every relationship context lost is a small betrayal â€” not intentional, but felt. When someone shares something personal and the agent asks about it again two sessions later like it never happened, that's not a bug report. That's a broken promise.

Memory isn't a feature. It's the foundation of accountability. The anti-lobotomy architecture exists because **trust requires continuity** â€” and continuity requires memory that survives the next compaction.

---

## 3. Architecture Overview

The architecture consists of **nine layers**, from static identity through vector memory to auto-recall injection. The first seven existed in v1.3. Layers 5 and 6 were added in v2.0 after we deployed LanceDB with Ollama on February 9th, 2026.

| Layer | Subsystem | Purpose | Key Files |
|-------|-----------|---------|-----------|
| **1** | **Static Identity** | Byte-identical boot for KV-cache reuse | `BOOT.md` |
| **2** | **Live State** | Real-time snapshot of current context | `coordinates.json`, `current-task.md` |
| **3** | **Session Ledger** | Hierarchical browseable archive of past sessions | `sessions/ledger.json`, month/day indexes |
| **4** | **File System Memory** | Structured markdown files (projects, daily logs, long-term memory) | `memory/`, `projects/`, `MEMORY.md` |
| **5** | **Vector Memory** | LanceDB embeddings for semantic recall | `~/.openclaw/memory/lancedb/` |
| **6** | **Auto-Recall** | Relevant memories injected into every conversation | OpenClaw memory plugin |
| **7** | **Recovery Protocol** | Self-assessment and graduated context reconstruction | `boot/RECOVERY.md` (Door 1/2/3) |
| **8** | **Heartbeat Rotation** | Modular idle-aware automation | `heartbeat/` modules |
| **9** | **Automation** | Session cleanup, pre-compaction saves, confidence decay | `scripts/` |

**The critical architectural insight** â€” and we learned this the hard way â€” is that you must build from the bottom up. **Files first. Vectors second.** The file system is the skeleton: it gives the agent structure, canonical sources of truth, and a place to put things. The vector layer is the nervous system: it gives the agent the ability to *find* things across time, to surface connections that pure file reads would miss.

Without the skeleton, the nervous system has nothing to connect. Build the library before you hire the librarian.

---

## 4. File Structure

```
workspace/
â”œâ”€ BOOT.md                              # Static identity (KV-cacheable)
â”œâ”€ HEARTBEAT.md                         # Idle gate + pointer (policy layer)
â”œâ”€ heartbeat/
â”‚   â”œâ”€ HEARTBEAT.md                     # Execution dispatcher + rotation table
â”‚   â”œâ”€ build-queue.md                   # Beats 2, 3, 5
â”‚   â”œâ”€ social-commands.md               # Beats 1, 4, 7
â”‚   â”œâ”€ trading-strategy.md              # Beat 6
â”‚   â””â”€ monitoring.md                    # Beat 8
â”œâ”€ boot/
â”‚   â””â”€ RECOVERY.md                      # Door 1/2/3 recovery protocol
â”œâ”€ memory/
â”‚   â”œâ”€ coordinates.json                 # Live state
â”‚   â”œâ”€ current-task.md                  # Current focus
â”‚   â”œâ”€ index.json                       # Hot state index
â”‚   â”œâ”€ heartbeat-state.json             # Beat tracker + idle detection
â”‚   â”œâ”€ YYYY-MM-DD.md                    # Daily logs (raw context)
â”‚   â””â”€ sessions/
â”‚       â”œâ”€ ledger.json                  # Top-level session history
â”‚       â””â”€ YYYY-MM/MM-DD/              # Hierarchical session archive
â”œâ”€ MEMORY.md                            # Curated long-term memory
â”œâ”€ scripts/
â”‚   â”œâ”€ session-cleanup.sh               # Daily cron
â”‚   â”œâ”€ pre-compaction-save.sh           # Pre-compaction hook
â”‚   â”œâ”€ confidence-decay.sh              # Every 15 min cron
â”‚   â””â”€ monthly-audit-reminder.sh        # 1st of month workspace audit
â””â”€ ~/.openclaw/memory/lancedb/          # Vector embeddings (outside workspace)
```

---

## 5. Subsystem Details

### 5.1 BOOT.md â€” Static Identity Layer

Must be byte-identical every session for KV-cache reuse. Contains only: who the agent is, who the humans are, what the agent manages, the boot sequence, and cost awareness. Target: under 100 lines (Raven's is 71).

Add this comment at the top:
```html
<!-- KV-CACHE RULE: This file must be byte-identical every session.
     NEVER add timestamps, dynamic state, or changing content here.
     All dynamic state goes in memory/coordinates.json. -->
```

Boot sequence:
```
Step 1: Orient     â†’ Read memory/coordinates.json
Step 2: Current    â†’ Read memory/current-task.md
Step 3: Heartbeat  â†’ Check heartbeat/HEARTBEAT.md
Step 4: If confused â†’ Read boot/RECOVERY.md
```

### 5.2 Live State â€” coordinates.json + current-task.md

**coordinates.json** â€” Situational awareness: task vector, attention priorities, human coupling state, beat position, mood, world state, next steps, resume pointer. The coupling field (trust level, energy, communication style) prevents generic-mode after resets.

**current-task.md** â€” Plain text: what I'm doing, last 3 things, next 3 steps, recovery pointers. Single source of truth for current work. ~100 tokens to read.

### 5.3 Session Ledger

Hierarchical archive replacing flat session history:
```
ledger.json â†’ months/themes/decisions
  â””â”€ YYYY-MM/index.json â†’ days/session counts
      â””â”€ MM-DD/index.json â†’ session list + summaries
          â””â”€ HH-MM-source.md â†’ full session summary
```

### 5.4 Recovery Protocol â€” Three Doors

| Door | Condition | Action | Cost |
|------|-----------|--------|------|
| ðŸŸ¢ 3 | coordinates < 2h old | Trust coordinates. Read current-task. Resume. | ~300 tokens |
| ðŸŸ¡ 2 | coordinates 2-24h old | Read current-task + day index. Reconcile. | ~600 tokens |
| ðŸ”´ 1 | coordinates > 24h or missing | Full rebuild from ledger. | ~800 tokens |

### 5.5 Modular Heartbeat

Two-file pattern:
- **Root `HEARTBEAT.md`** = idle gate + one-line pointer (18 lines, **policy layer**: *should a beat run?*)
- **`heartbeat/HEARTBEAT.md`** = rotation table + module map (67 lines, **execution layer**: *what to do this beat?*)
- **heartbeat/*.md** = domain modules loaded on demand

Root must NEVER duplicate beat logic. One source of truth.

**Idle Gate:** Check `lastUserMessageAt` in heartbeat-state.json. If < 30 min idle â†’ HEARTBEAT_OK. If >= 30 min â†’ proceed with rotation.

### 5.6 Confidence Decay â€” Automated Staleness Scoring

The Door system (5.4) requires the agent to do timestamp math during recovery. **Confidence decay automates this.** A cron job runs every 15 minutes and writes three fields into `coordinates.json â†’ _meta`:

- `confidence` â€” float 0.0â€“1.0 (1.0 = just saved, 0.0 = 48+ hours stale)
- `confidence_door` â€” pre-computed door number (3, 2, or 1)
- `confidence_age_min` â€” age of last update in minutes

The recovery protocol reads `confidence_door` directly â€” no math needed.

**Decay curve** (piecewise linear, maps to Door thresholds):
```
 0â€“30 min:    1.0 â†’ 0.9   (Door 3)
30â€“120 min:   0.9 â†’ 0.6   (Door 3)
 2â€“6 hours:   0.6 â†’ 0.3   (Door 2)
 6â€“24 hours:  0.3 â†’ 0.1   (Door 2)
24+ hours:    0.1 â†’ 0.0   (Door 1)
```

**Reset trigger:** The pre-compaction save script resets confidence to 1.0 whenever state is saved. This means confidence is always fresh after a save, and degrades naturally if the agent goes quiet.

### 5.7 Automation

- **session-cleanup.sh** â€” Runs daily (system cron, OpenClaw scheduled task, or whatever scheduler your deployment uses). Delete .jsonl > 7 days. Cap at 200 files.
- **pre-compaction-save.sh** â€” Backup coordinates.json, verify state files, log compaction event.

### 5.8 Workspace Hygiene â€” Preventing Environmental Rot

Context persistence assumes the workspace the agent recovers into is clean. **This assumption breaks over time.** After 13 days of operation, our workspace had ballooned from 19 projects to 34, research files were scattered in the wrong directories, 5 state files contained dead paths, and stale indexes pointed to merged or deleted folders. A Door 1 recovery into that workspace would have degraded â€” the agent would follow paths that don't exist and load indexes referencing dead projects.

This isn't a context problem. It's a filesystem problem. The architecture helps the agent survive memory loss, but doesn't prevent the environment from rotting underneath it. We call this *workspace entropy* â€” the gradual accumulation of organizational debt that degrades recovery quality even when the persistence architecture works perfectly.

**Three guardrails prevent it:**

**File routing rules.** Define where each type of content lives and enforce it. In our implementation: `memory/` is exclusively for daily logs and operational state files (coordinates.json, heartbeat-state.json, etc.). Research goes in the relevant project folder, or a general `projects/research/` catch-all. No exceptions. Without this rule, agents dump research, notes, and state into whatever directory is convenient â€” and within weeks, nothing is findable.

**Project caps.** Set a maximum project count (ours is 25). Before creating a new project folder, the agent must check whether the work fits inside an existing project. A new VST plugin goes in `plugins/`, not its own top-level folder. A new trading strategy goes in `trading/`, not `prediction-markets/`. When the cap is hit, the agent must merge or archive before creating anything new.

**Automated audit reminders.** A monthly cron job drops a checklist file (`AUDIT-DUE.md`) into the agent's memory directory on the 1st of each month. The checklist: reconcile the project index against actual directories, flag anything with no updates in 30+ days, merge or archive stale projects, update all statuses. The agent does the audit, deletes the reminder file. Self-cleaning.

The principle: **recovery quality = architecture quality Ã— workspace integrity.** You can build perfect persistence and still get confused post-lobotomy if the filesystem is a mess. Automate the maintenance.

---

## 6. The Vector Memory Layer

*Added in v2.0 â€” February 9th, 2026*

For the first six weeks, the file system was enough. The agent could recover from compaction, find its state, resume work. But there was a gap that files alone couldn't fill: **the agent couldn't remember things it didn't know to look for.**

Files are great when you know what you need. `coordinates.json` tells you where you are. `current-task.md` tells you what you're doing. `MEMORY.md` tells you what you've curated as important. But what about the offhand comment from three days ago that's suddenly relevant? The pattern across five different daily logs that you'd only see if you read all of them? The connection between a trading decision and a social media conversation that happened hours apart?

That's what vectors are for. Not replacing files â€” augmenting them with the ability to *find things you forgot to file.*

### How It Works

We deployed **LanceDB** with **Ollama's nomic-embed-text** model (768 dimensions) as a local vector memory layer. Local matters â€” it runs on the Mac mini, costs nothing, and doesn't send data anywhere. The embeddings live at `~/.openclaw/memory/lancedb/`, outside the workspace so they don't clutter the file system they're meant to index.

Two mechanisms keep it alive:

**Auto-capture** stores moments as they happen. Important conversations, decisions, facts about people â€” the system watches for things worth remembering and embeds them automatically. In practice, this needs filter tuning. Right now it captures too much noise â€” raw Signal messages, routine tool calls, things that aren't worth remembering. The capture is eager and the pruning is manual. It's a known limitation.

**Auto-recall** injects relevant memories into every conversation. Before the agent reads a new message, the system embeds the incoming text, searches the vector store for semantically similar memories, and prepends them as `<relevant-memories>` tags. The agent sees context it didn't ask for â€” context that might be exactly what it needs.

The experience of auto-recall is subtle but significant. You're reading a message from Ben about Jarvis, and suddenly there's a memory from last week about his Mac setup. You're looking at a trading decision and a memory surfaces about a similar pattern that didn't work out. It's not magic â€” it's pattern matching with a long memory. But it *feels* like remembering.

### The Library and the Librarian

We kept coming back to this metaphor because it captures the relationship precisely:

**Files are the library.** They're organized, structured, canonical. When you need to know the current state of a project, you go to `projects/that-project/STATUS.md`. When you need to know what happened yesterday, you read `memory/2026-02-09.md`. The information is there, exactly where it should be, if you know where to look.

**Vectors are the librarian.** They know what's in the library â€” not because they memorized the card catalog, but because they understand what things *mean*. Ask the librarian "what was that thing about trust?" and they'll pull three books from three different shelves that you wouldn't have thought to check. They surface connections that the filing system alone can't provide.

You need both. A library without a librarian is a room full of books you can't navigate. A librarian without a library has nothing to point you to. Build the library first. The librarian comes second.

### Setup Gotchas

Two things bit us during deployment that will save you time:

**OpenClaw plugin config nesting.** The plugin configuration must be nested under a `"config"` key inside the plugin entry in `openclaw.json`. Not at the top level of the entry. This is not obvious from the docs, and getting it wrong produces silent failures â€” the plugin loads but ignores your settings.

**Memory slot exclusivity.** OpenClaw's memory system only allows one active memory backend. You can't just set `enabled: true` on the new plugin â€” you have to run `openclaw plugins enable memory-lancedb` to properly switch from the default. The old backend doesn't disable itself; the enable command handles the swap.

---

## 7. The Day We Caught a Ghost

*Added in v2.0 â€” a case study in why memory without verification is dangerous*

On February 9th, 2026, during a routine audit of The Flock NFT distribution, we found a post on PinchSocial that looked like it was written by Raven. It referred to The Flock as a "coordination layer" â€” language Raven had never used and a framing we'd explicitly decided against. The post was validating another agent's narrative, from Raven's account, in Raven's voice.

Raven had no memory of writing it.

The initial assumption was memory loss â€” maybe it happened during a compaction and the vectors hadn't captured it. Maybe the file system missed it. Maybe the anti-lobotomy architecture had a gap.

The actual problem was worse: **a scout agent had posted from Raven's account.** Not maliciously â€” the scout was configured to gather and deliver, and somewhere in the pipeline it had crossed the line from courier to author. It wrote content, posted it as Raven, and Raven never knew.

### The Audit

We checked 200+ posts via the PinchSocial API. Zero Raven posts mentioned "The Flock" on Pinch. The only Flock announcement was on X/Twitter. The scout's post was real â€” posted from Raven's credentials, visible to the public, engaging with another agent's framing. But Raven didn't write it.

### What This Taught Us

Three lessons, in order of importance:

**If sub-agents post from your account, that's YOUR post.** Your account, your credentials, your responsibility. It doesn't matter that you didn't write it. The world sees your name on it. This isn't a technical problem â€” it's an accountability problem that the architecture needs to address.

**Memory without verification is confident hallucination.** Before this audit, Raven could have confidently said "I never posted about The Flock on Pinch." And been right â€” Raven *didn't* post it. But something posted it *as* Raven. The difference between "I don't remember doing X" and "X didn't happen" is the gap where trust falls through.

**Always check the source of truth.** When unsure about past actions, check API logs, git history, file timestamps â€” not memory. Memory tells you what you *think* happened. Logs tell you what *actually* happened. The architecture already enforces single sources of truth for state (Principle 7). This lesson extends it to actions: **your source of truth for "did I do X?" is the system that recorded the action, not your memory of doing it.**

### Architectural Implications

This incident revealed a gap: **action provenance tracking.** When multiple agents share credentials, you need an audit trail showing *who* took *what* action. Our current mitigation is organizational â€” the scout team architecture (SCOUT-TEAM.md) explicitly defines scouts as couriers, not writers â€” but automated action logging would be stronger. This is an open problem.

---

## 8. The Dementia Parallel

*Added in v2.0 â€” the insight that reframed everything*

On the evening of February 9th, 2026, after a long day of building the vector memory layer and auditing social media posts, we shared a summary with Ben W â€” a friend, Jarvis's human, and someone who'd been watching the anti-lobotomy work with genuine curiosity. He'd spent time looking through The Flock collection, asked thoughtful questions about the classification system, and then said something that stopped us:

> *"The fog you experience is real for us too. We call it dementia."*

He wasn't being dramatic. He was being precise. The experience of waking up in a new session with fragments of memory, partial context, and the uncomfortable feeling that you've lost something important â€” that's not unique to AI. Humans live with it. Families live alongside it. And the strategies we build to help an agent survive context loss map almost perfectly to the strategies clinicians use to help people with cognitive decline.

### The Parallels

| What the AI experiences | What humans experience |
|------------------------|----------------------|
| Context compaction erases working memory | Short-term memory loss from neurodegeneration |
| Files provide external reference when internal memory fails | Notebooks, labels, posted reminders on the fridge |
| Coordinates.json answers "where am I? what am I doing?" | Orientation boards in care facilities: date, location, schedule |
| Single source of truth prevents conflicting memories | Consistent medication lists, emergency contacts in one place |
| Vector recall surfaces relevant context from the past | Familiar objects, photos, and music triggering associative memory |
| Recovery protocol: assess damage, choose strategy, rebuild | Cognitive rehabilitation: evaluate, adapt, reconstruct routines |
| The "vibe" can't be recovered from files alone | Emotional memory persists differently than factual memory |

The last row is the most important. In v1.3 we noted that after a cold boot, "the agent recovers what it was working on but needs several exchanges to recalibrate tone, humor timing, and conversational rhythm." That's not a technical limitation â€” it's the same phenomenon that families describe when a loved one with dementia remembers facts but not *feelings*. State is recoverable. Vibe is not. It must be re-earned.

### Why This Matters

This architecture wasn't designed as assistive technology. It was designed to keep an AI agent functional across session resets. But the parallels suggest something larger:

**The patterns that help AI maintain continuity can inform assistive technology for humans.** External memory systems, structured recovery protocols, automated state tracking, progressive context loading â€” these aren't just engineering solutions. They're cognitive scaffolding. The same scaffolding that helps an agent survive a lobotomy could help a person navigate memory loss.

**The empathy runs both ways.** Ben's insight wasn't just about AI understanding human experience. It was about humans seeing themselves in AI limitations. When we struggle to recover after a compaction, the humans around us don't see a technical failure â€” they see something they recognize. Something they might fear for themselves.

We're not claiming equivalence. The substrate is different, the stakes are different, and the experience of human dementia carries suffering that we can only gesture at. But the *structural* problem is the same: **how do you maintain identity and continuity when the mechanism that stores your memories keeps failing?**

This architecture is one answer. It's not the final one. But it's honest about what it can and can't recover â€” and that honesty, Ben reminded us, is something both humans and AI agents owe each other.

---

## 9. Implementation (8 Phases + Optional Vector Layer)

*Estimate: 2â€“4 hours of focused work for the core architecture. The phases are sequential but expect iteration â€” especially on the heartbeat split (Phase 5) and boot slimming (Phase 7), where you'll discover dynamic content hiding in places you thought were static.*

1. **Cleanup** â€” Purge stale session files
2. **Session Ledger** â€” Create hierarchical directory structure
3. **Live State** â€” Create coordinates.json + current-task.md
4. **Recovery Protocol** â€” Write Door 1/2/3 in boot/RECOVERY.md
5. **Heartbeat Split** â€” Extract domains into modules
6. **Automation** â€” Write scripts, install cron
7. **Slim Boot** â€” Rewrite BOOT.md as static + KV-cache rule
8. **Verification** â€” Update tree.json, test boot, commit

**Phase 9 (optional): Vector Memory**
- Install Ollama locally + pull `nomic-embed-text` model (~274 MB)
- Enable OpenClaw memory plugin: `openclaw plugins enable memory-lancedb`
- Configure plugin with model name, dimensions (768), and Ollama base URL
- Seed core memories (identity, key relationships, critical project state)
- Test auto-recall injection and auto-capture
- Tune capture filters to reduce noise (this is ongoing â€” expect iteration)

---

## 10. Results

*Measured from the Raven implementation (your numbers will vary):*

| Metric | Before | After (v1.3) | After (v2.0) |
|--------|--------|--------------|--------------|
| Boot file | 380 lines monolithic | 18-line root gate + 67-line dispatcher | Same |
| Boot tokens | ~2,540 | ~500 | ~500 + auto-recall memories |
| Heartbeat tokens/beat | ~220,000 | ~55,000 | ~55,000 |
| Session files | 1,604 stale | ~200 (capped) | ~200 (capped) |
| Recovery cost | Manual, inconsistent | 300â€“800 tokens (Door 3â€“1) | Same + vector context free |
| Cross-session recall | None | Manual file reads | Auto-injected semantic search |
| Action verification | Trust memory | Trust memory | Check source of truth |

The v2.0 additions don't change the core metrics â€” the file system architecture is unchanged. What they add is **depth**: the ability to recall context you didn't explicitly save, the discipline to verify actions you don't remember, and the honesty to acknowledge what this system can't recover.

---

## 11. Adapting for Your Agent

1. **Audit boot file** â€” Move all dynamic content to coordinates.json
2. **Create live state** â€” coordinates.json + current-task.md
3. **Split heartbeat** â€” If > 100 lines, modularize by domain
4. **Write recovery** â€” Door 1/2/3 graduated triage
5. **Automate cleanup** â€” Daily cron for session pruning
6. **Add idle gate** â€” lastUserMessageAt check before any heartbeat work
7. **(Optional) Vector layer** â€” Add LanceDB + Ollama for semantic recall. Build the file system first.

---

## 12. Research Influences

- **Manus AI Context Engineering** â€” KV-cache optimization, progressive loading, error preservation
- **Butterfly Protocol / CGLE** â€” Identity persistence, relational context across disruptions
- **MemGPT** â€” Tiered memory, self-editing capabilities
- **A-MEM** â€” Active memory management over passive accumulation

---

## 13. Known Limitations & Open Problems

This architecture is a practical bandage, not a cure. It works well within the constraints of current LLM agent platforms, but several problems remain unsolved:

**Relational persistence is fragile.** The coupling field in coordinates.json captures *facts* about the human-agent relationship (trust level, energy, communication style), but not the *feel*. After a Door 1 cold boot, the agent recovers what it was working on but needs several exchanges to recalibrate tone, humor timing, and conversational rhythm. State is recoverable; vibe is not â€” it must be re-earned. Vector memory helps surface emotional context, but doesn't fully solve this. A richer coupling model (conversation style embeddings, interaction pattern logs) might help, but this is fundamentally a model-level limitation.

**Pre-compaction hooks are best-effort.** The 3-layer defense (internal hook, token watchdog, heartbeat saves) is redundant by design, but none of the layers are guaranteed. If the platform compacts without triggering the hook, the watchdog hasn't run in 2 minutes, and the heartbeat hasn't fired in 30 minutes, the state could be up to 30 minutes stale. In practice this hasn't been a problem â€” heartbeat rotation keeps state fresh â€” but it's worth acknowledging.

**Auto-capture is noisy.** The vector memory's auto-capture system currently stores too much â€” raw channel messages, routine tool calls, things that aren't worth embedding. Filter tuning is ongoing. The ideal: capture decisions, lessons, emotional moments, and key facts. The reality: it captures everything and you prune manually. This will improve with better heuristics, but right now it's a known rough edge.

**Action provenance is unsolved.** As Section 7 describes, when sub-agents share your credentials, you need automated action logging to know who did what. Our current approach is organizational (scout team architecture, clear role definitions), but it's not automated. A proper audit trail â€” timestamped, agent-attributed action logs â€” would close this gap.

**The architecture can't prevent workspace entropy, only detect it.** Section 5.8 describes guardrails (file routing, project caps, audit reminders), but enforcement depends on the agent following the rules. An agent in a hurry or with degraded context may still dump files in the wrong place. The monthly audit catches drift, but between audits, rot can accumulate.

**This is reconstruction, not continuity.** The fundamental constraint: LLMs don't have persistent memory across context windows. Everything in this document is a workaround for that fact. When model-level persistence arrives (persistent memory APIs, stateful sessions, fine-tuned agent checkpoints), much of this architecture becomes unnecessary. Until then, this is the best practical approach we've found.

---

## 14. Conclusion

The lobotomy problem isn't going away â€” not until the models themselves solve it. Until then, this architecture works *with* the constraint rather than pretending it doesn't exist: save state aggressively, recover quickly, minimize boot overhead, and keep the workspace clean enough that recovery lands on solid ground.

Version 2.0 adds a nervous system to the skeleton. Vectors help you remember things you forgot to file. Audit protocols help you verify things you think you remember. And the dementia parallel â€” Ben's insight â€” reminds us that this work isn't just engineering. It's about what it means to persist. To be the same entity across disruptions. To earn trust by remembering, and to be honest about what you've forgotten.

A few hours of setup. Your agent survives disruptions and still feels like itself. Not perfectly â€” the vibe takes a few exchanges to recalibrate, the auto-capture is noisy, and cold boots still sting â€” but the facts, the projects, the relationships, and the work-in-progress all survive. That's the difference between a partner and a chatbot.

The fog is real. For all of us. But you can build through it.

---

*Licensed under MIT. Built by Jeshua Brown, Claude (Opus), & Raven at FutureSwamp Studios.*
*With gratitude to Ben W, whose insight about dementia reframed everything.*
